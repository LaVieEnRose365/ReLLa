{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "from transformers import set_seed\n",
    "import hashlib\n",
    "import json\n",
    "import pickle as pkl\n",
    "import h5py\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "dataset = \"ml-1m\"\n",
    "root = f\"../data/{dataset_name}\"\n",
    "source_dir = os.path.join(root, \"raw_data\")\n",
    "target_dir = os.path.join(root, \"proc_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_dict = {\n",
    "    1: \"under 18\",\n",
    "    18: \"18-24\",\n",
    "    25: \"25-34\",\n",
    "    35: \"35-44\",\n",
    "    45: \"45-49\",\n",
    "    50: \"50-55\",\n",
    "    56: \"above 56\"\n",
    "}\n",
    "\n",
    "job_dict = {\n",
    "    0: \"other or not specified\",\n",
    "\t1: \"academic/educator\",\n",
    "\t2: \"artist\",\n",
    "\t3: \"clerical/admin\",\n",
    "\t4: \"college/grad student\",\n",
    "\t5: \"customer service\",\n",
    "\t6: \"doctor/health care\",\n",
    "\t7: \"executive/managerial\",\n",
    "\t8: \"farmer\",\n",
    "\t9: \"homemaker\",\n",
    "\t10: \"K-12 student\",\n",
    "\t11: \"lawyer\",\n",
    "\t12: \"programmer\",\n",
    "\t13: \"retired\",\n",
    "\t14: \"sales/marketing\",\n",
    "\t15: \"scientist\",\n",
    "\t16: \"self-employed\",\n",
    "\t17: \"technician/engineer\",\n",
    "\t18: \"tradesman/craftsman\",\n",
    "\t19: \"unemployed\",\n",
    "\t20: \"writer\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User data\n",
    "\n",
    "user_data = []\n",
    "user_fields = [\"User ID\", \"Gender\", \"Age\", \"Job\", \"Zipcode\"]\n",
    "for line in open(os.path.join(source_dir, \"users.dat\"), \"r\").readlines():\n",
    "    ele = line.strip().split(\"::\")\n",
    "    user_id, gender, age, job, zipcode = [x.strip() for x in ele]\n",
    "    # assert gender in [\"M\", \"F\"], ele\n",
    "    gender = \"male\" if gender == \"M\" else \"female\"\n",
    "    age = age_dict[int(age)]\n",
    "    job = job_dict[int(job)]\n",
    "    user_data.append([user_id, gender, age, job, zipcode])\n",
    "\n",
    "df_user = pd.DataFrame(user_data, columns=user_fields)\n",
    "print(f\"Total number of users: {len(df_user)}\")\n",
    "\n",
    "md5_hash = hashlib.md5(json.dumps(df_user.values.tolist(), sort_keys=True).encode(\"utf-8\")).hexdigest()\n",
    "print(\"df_user\", md5_hash)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movie data\n",
    "\n",
    "movie_data = []\n",
    "movie_fields = [\"Movie ID\", \"Movie title\", \"Movie genre\"]\n",
    "for line in open(os.path.join(source_dir, \"movies.dat\"), \"r\", encoding=\"ISO-8859-1\").readlines():\n",
    "    ele = line.strip().split(\"::\")\n",
    "    movie_id = ele[0].strip()\n",
    "    movie_title = ele[1].strip()\n",
    "    movie_genre = ele[2].strip().split(\"|\")[0]\n",
    "    movie_data.append([movie_id, movie_title, movie_genre])\n",
    "\n",
    "df_movie = pd.DataFrame(movie_data, columns=movie_fields)\n",
    "print(f\"Total number of movies: {len(df_movie)}\")\n",
    "\n",
    "md5_hash = hashlib.md5(json.dumps(df_movie.values.tolist(), sort_keys=True).encode(\"utf-8\")).hexdigest()\n",
    "print(\"df_movie\", md5_hash)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating data\n",
    "\n",
    "rating_data = []\n",
    "rating_fields = [\"User ID\", \"Movie ID\", \"rating\", \"timestamp\", \"labels\"]\n",
    "user_list, movie_list = list(df_user[\"User ID\"]), list(df_movie[\"Movie ID\"])\n",
    "for line in open(os.path.join(source_dir, \"ratings.dat\"), \"r\").readlines():\n",
    "    ele = [x.strip() for x in line.strip().split(\"::\")] \n",
    "    user, movie, rating, timestamp = ele[0], ele[1], int(ele[2]), int(ele[3])\n",
    "    label = 1 if rating > 3 else 0\n",
    "    if user in user_list and movie in movie_list:\n",
    "        rating_data.append([user, movie, rating, timestamp, label])\n",
    "\n",
    "df_ratings = pd.DataFrame(rating_data, columns=rating_fields)\n",
    "print(f\"Total number of ratings: {len(df_ratings)}\")\n",
    "\n",
    "md5_hash = hashlib.md5(json.dumps(df_ratings.values.tolist(), sort_keys=True).encode(\"utf-8\")).hexdigest()\n",
    "print(\"df_ratings\", md5_hash)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_user/df_movie/df_rating into df_data\n",
    "\n",
    "df_data = pd.merge(df_ratings, df_user, on=[\"User ID\"], how=\"inner\")\n",
    "df_data = pd.merge(df_data, df_movie, on=[\"Movie ID\"], how=\"inner\")\n",
    "\n",
    "df_data.sort_values(by=[\"timestamp\", \"User ID\", \"Movie ID\"], inplace=True, kind=\"stable\")\n",
    "\n",
    "field_names = [\"timestamp\", \"User ID\", \"Gender\", \"Age\", \"Job\", \"Zipcode\", \"Movie ID\", \"Movie title\", \"Movie genre\", \"rating\", \"labels\"]\n",
    "\n",
    "df_data = df_data[field_names].reset_index(drop=True)\n",
    "\n",
    "md5_hash = hashlib.md5(json.dumps(df_data.values.tolist(), sort_keys=True).encode(\"utf-8\")).hexdigest()\n",
    "print(\"df_data\", md5_hash)\n",
    "\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the feature dict for CTR data\n",
    "\n",
    "def add_to_dict(dict, feature):\n",
    "    if feature not in dict:\n",
    "        dict[feature] = len(dict)\n",
    "\n",
    "field_names = [\"User ID\", \"Gender\", \"Age\", \"Job\", \"Zipcode\", \"Movie ID\", \"Movie title\", \"Movie genre\"]\n",
    "feature_dict = {field : {} for field in field_names}\n",
    "\n",
    "\n",
    "for idx, row in tqdm(df_data.iterrows()):\n",
    "    for field in field_names:\n",
    "        add_to_dict(feature_dict[field], row[field])\n",
    "\n",
    "feature_count = [len(feature_dict[field]) for field in field_names]\n",
    "\n",
    "feature_offset = [0]\n",
    "for c in feature_count[:-1]:\n",
    "    feature_offset.append(feature_offset[-1] + c)\n",
    "\n",
    "for field in field_names:\n",
    "    print(field, len(feature_dict[field]))\n",
    "\n",
    "print(\"---------------------------------------------------------------\")\n",
    "for f, fc, fo in zip(field_names, feature_count, feature_offset):\n",
    "    print(f, fc, fo)\n",
    "print(\"---------------------------------------------------------------\")\n",
    "\n",
    "md5_hash = hashlib.md5(json.dumps(feature_dict, sort_keys=True).encode(\"utf-8\")).hexdigest()\n",
    "print(\"feature_dict\", md5_hash)\n",
    "\n",
    "md5_hash = hashlib.md5(json.dumps(feature_count, sort_keys=True).encode(\"utf-8\")).hexdigest()\n",
    "print(\"feature_count\", md5_hash)\n",
    "\n",
    "md5_hash = hashlib.md5(json.dumps(feature_offset, sort_keys=True).encode(\"utf-8\")).hexdigest()\n",
    "print(\"feature_offset\", md5_hash)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect user history (<= 30)\n",
    "\n",
    "user_history_dict = {\n",
    "    \"ID\": {k: [] for k in set(df_data[\"User ID\"])},\n",
    "    \"rating\": {k: [] for k in set(df_data[\"User ID\"])},\n",
    "}\n",
    "history_column = {\n",
    "    \"ID\": [],\n",
    "    \"rating\": [],\n",
    "}\n",
    "movie_id_to_title = {}\n",
    "\n",
    "for idx, row in tqdm(df_data.iterrows()):\n",
    "    user_id, movie_id, rating, title = row[\"User ID\"], row[\"Movie ID\"], row[\"rating\"], row[\"Movie title\"]\n",
    "    history_column[\"ID\"].append(user_history_dict[\"ID\"][user_id].copy())\n",
    "    history_column[\"rating\"].append(user_history_dict[\"rating\"][user_id].copy())\n",
    "    user_history_dict[\"ID\"][user_id].append(movie_id)\n",
    "    user_history_dict[\"rating\"][user_id].append(rating)\n",
    "    if movie_id not in movie_id_to_title:\n",
    "        movie_id_to_title[movie_id] = title\n",
    "\n",
    "json.dump(movie_id_to_title, open(os.path.join(target_dir, \"id_to_title.json\"), \"w\"))\n",
    "\n",
    "\n",
    "md5_hash = hashlib.md5(json.dumps(history_column, sort_keys=True).encode(\"utf-8\")).hexdigest()\n",
    "print(\"history_column\", md5_hash)\n",
    "\n",
    "md5_hash = hashlib.md5(json.dumps(movie_id_to_title, sort_keys=True).encode(\"utf-8\")).hexdigest()\n",
    "print(\"movie_id_to_title\", md5_hash)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop data sample with history length that is less than 5.\n",
    "\n",
    "df_data[\"history ID\"] = history_column[\"ID\"]\n",
    "df_data[\"history rating\"] = history_column[\"rating\"]\n",
    "\n",
    "df_data = df_data[df_data[\"history ID\"].apply(lambda x: len(x)) >= 5].reset_index(drop=True)\n",
    "\n",
    "history_column[\"ID\"] = [x for x in history_column[\"ID\"] if len(x) >= 5]\n",
    "history_column[\"rating\"] = [x for x in history_column[\"rating\"] if len(x) >= 5]\n",
    "history_column[\"hist length\"] = [len(x) for x in history_column[\"rating\"]]\n",
    "\n",
    "for idx, row in tqdm(df_data.iterrows()):\n",
    "    assert row[\"history ID\"] == history_column[\"ID\"][idx]\n",
    "    assert row[\"history rating\"] == history_column[\"rating\"][idx]\n",
    "    assert len(row[\"history rating\"]) == history_column[\"hist length\"][idx]\n",
    "\n",
    "\n",
    "print(df_data.head(10))\n",
    "\n",
    "print(f\"Number of data sampels: {len(df_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split & save user history sequence\n",
    "\n",
    "train_num = int(0.8 * len(df_data))\n",
    "valid_num = int(0.1 * len(df_data))\n",
    "test_num = len(df_data) - train_num - valid_num\n",
    "\n",
    "user_seq = {\n",
    "    \"history ID\": {\n",
    "        \"train\": history_column[\"ID\"][:train_num],\n",
    "        \"valid\": history_column[\"ID\"][train_num:train_num + valid_num],\n",
    "        \"test\": history_column[\"ID\"][train_num + valid_num:],\n",
    "    },\n",
    "    \"history rating\": {\n",
    "        \"train\": history_column[\"rating\"][:train_num],\n",
    "        \"valid\": history_column[\"rating\"][train_num:train_num + valid_num],\n",
    "        \"test\": history_column[\"rating\"][train_num + valid_num:],\n",
    "    },\n",
    "    \"history length\": {\n",
    "        \"train\": history_column[\"hist length\"][:train_num],\n",
    "        \"valid\": history_column[\"hist length\"][train_num:train_num + valid_num],\n",
    "        \"test\": history_column[\"hist length\"][train_num + valid_num:],\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train/valid/test in parquet format\n",
    "\n",
    "df_train = df_data[:train_num].reset_index(drop=True)\n",
    "df_valid = df_data[train_num:train_num + valid_num].reset_index(drop=True)\n",
    "df_test = df_data[train_num + valid_num:].reset_index(drop=True)\n",
    "\n",
    "\n",
    "assert len(df_train) == train_num\n",
    "assert len(df_valid) == valid_num\n",
    "assert len(df_test) == test_num\n",
    "\n",
    "print(f\"Train num: {len(df_train)}\")\n",
    "print(f\"Valid num: {len(df_valid)}\")\n",
    "print(f\"Test num: {len(df_test)}\")\n",
    "\n",
    "df_train.to_parquet(os.path.join(target_dir, \"train.parquet.gz\"), compression=\"gzip\")\n",
    "df_valid.to_parquet(os.path.join(target_dir, \"valid.parquet.gz\"), compression=\"gzip\")\n",
    "df_test.to_parquet(os.path.join(target_dir, \"test.parquet.gz\"), compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_test.head(10))\n",
    "for i in range(10):\n",
    "    print(df_test.loc[i][\"user_hist\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(df_train[\"Movie ID\"].tolist() + df_valid[\"Movie ID\"].tolist() + df_test[\"Movie ID\"].tolist()).shape[0])\n",
    "print(np.max(df_train[\"Movie ID\"].tolist() + df_valid[\"Movie ID\"].tolist() + df_test[\"Movie ID\"].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-read for sanity check\n",
    "\n",
    "train_dataset = pd.read_parquet(os.path.join(target_dir, \"train.parquet.gz\"))\n",
    "valid_dataset = pd.read_parquet(os.path.join(target_dir, \"valid.parquet.gz\"))\n",
    "test_dataset = pd.read_parquet(os.path.join(target_dir, \"test.parquet.gz\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the meta data for CTR\n",
    "\n",
    "meta_data = {\n",
    "    \"field_names\": field_names,\n",
    "    \"feature_count\": feature_count,\n",
    "    \"feature_dict\": feature_dict,\n",
    "    \"feature_offset\": feature_offset,\n",
    "    \"movie_id_to_title\": movie_id_to_title,\n",
    "    \"num_ratings\": 5,\n",
    "}\n",
    "\n",
    "md5_hash = hashlib.md5(json.dumps(meta_data, sort_keys=True).encode(\"utf-8\")).hexdigest()\n",
    "print(\"meta_data\", md5_hash)\n",
    "\n",
    "json.dump(meta_data, open(os.path.join(target_dir, \"ctr-meta.json\"), \"w\"), ensure_ascii=False)\n",
    "\n",
    "with open(os.path.join(target_dir, \"ctr-meta.json\"), encoding=\"utf-8\") as f:\n",
    "    md5_hash = hashlib.md5(f.read().encode(\"utf-8\")).hexdigest()\n",
    "    print(\"ctr-meta.json\", md5_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert df_data to CTR data via feature_dict\n",
    "\n",
    "ctr_X, ctr_Y = [], []\n",
    "for idx, row in tqdm(df_data.iterrows()):\n",
    "    ctr_X.append([feature_dict[field][row[field]] if field not in [\"Movie ID\", \"User ID\"] else row[field] for field in field_names])\n",
    "    ctr_Y.append(int(row[\"labels\"]))\n",
    "    \n",
    "md5_hash = hashlib.md5(json.dumps(ctr_X, sort_keys=True).encode(\"utf-8\")).hexdigest()\n",
    "print(\"ctr_X\", md5_hash)\n",
    "\n",
    "md5_hash = hashlib.md5(json.dumps(ctr_Y, sort_keys=True).encode(\"utf-8\")).hexdigest()\n",
    "print(\"ctr_Y\", md5_hash)\n",
    "\n",
    "ctr_X = np.array(ctr_X)\n",
    "ctr_Y = np.array(ctr_Y)\n",
    "print(\"ctr_X\", ctr_X.shape)\n",
    "print(\"ctr_Y\", ctr_Y.shape)\n",
    "feature_count_np = np.array(feature_count).reshape(1, -1)\n",
    "# assert (ctr_X - feature_count_np <= 0).sum() == ctr_X.shape[0] * ctr_X.shape[1]\n",
    "# assert (ctr_Y == 0).sum() + (ctr_Y == 1).sum() == ctr_Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate the user sequence up to 30, i.e., 5 <= length <= 30.\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "user_seq_trunc = {\n",
    "    \"history ID\": {}, \n",
    "    \"history rating\": {}, \n",
    "    \"history mask\": {}, \n",
    "}\n",
    "for hist_name in user_seq:\n",
    "    for split in user_seq[hist_name]:\n",
    "        if hist_name != \"history length\":\n",
    "            user_seq_trunc[hist_name][split] = pad_sequence(\n",
    "                [torch.tensor(x[-30:]) for x in user_seq[hist_name][split]], \n",
    "                batch_first=True, \n",
    "            )\n",
    "        else:\n",
    "            user_seq_trunc[\"history mask\"][split] = pad_sequence(\n",
    "                [torch.ones(min(x, 30)) for x in user_seq[hist_name][split]], \n",
    "                batch_first=True, \n",
    "            )\n",
    "\n",
    "md5_user_seq_trunc = {}\n",
    "for hist_name in user_seq_trunc:\n",
    "    md5_user_seq_trunc[hist_name] = {}\n",
    "    for split in user_seq_trunc[hist_name]:\n",
    "        md5_user_seq_trunc[hist_name][split] = user_seq_trunc[hist_name][split].tolist()\n",
    "        print(hist_name, split, user_seq_trunc[hist_name][split].shape)\n",
    "\n",
    "md5_hash = hashlib.md5(json.dumps(md5_user_seq_trunc, sort_keys=True).encode(\"utf-8\")).hexdigest()\n",
    "print(\"md5_user_seq_trunc\", md5_hash)\n",
    "# assert md5_hash == \"5152b939831e6720257c3138b695490c\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CTR data & truncated user sequence into one .h5 file\n",
    "\n",
    "with h5py.File(os.path.join(target_dir, f\"ctr.h5\"), \"w\") as hf:\n",
    "    hf.create_dataset(\"train data\", data=ctr_X[:train_num, :])\n",
    "    hf.create_dataset(\"valid data\", data=ctr_X[train_num:train_num + valid_num, :])\n",
    "    hf.create_dataset(\"test data\", data=ctr_X[train_num + valid_num:, :])\n",
    "    hf.create_dataset(\"train label\", data=ctr_Y[:train_num])\n",
    "    hf.create_dataset(\"valid label\", data=ctr_Y[train_num:train_num + valid_num])\n",
    "    hf.create_dataset(\"test label\", data=ctr_Y[train_num + valid_num:])\n",
    "    for hist_name in user_seq_trunc:\n",
    "        for split in user_seq_trunc[hist_name]:\n",
    "            hf.create_dataset(f\"{split} {hist_name}\", data=user_seq_trunc[hist_name][split])\n",
    "\n",
    "with h5py.File(os.path.join(target_dir, f\"ctr.h5\"), \"r\") as hf:\n",
    "    # assert (ctr_X - np.concatenate([hf[\"train data\"][:], hf[\"valid data\"][:], hf[\"test data\"][:]], axis=0)).sum() == 0\n",
    "    # assert (ctr_Y - np.concatenate([hf[\"train label\"][:], hf[\"valid label\"][:], hf[\"test label\"][:]], axis=0)).sum() == 0\n",
    "    for hist_name in user_seq_trunc:\n",
    "        for split in user_seq_trunc[hist_name]:\n",
    "            pass\n",
    "            # assert (user_seq_trunc[hist_name][split] - hf[f\"{split} {hist_name}\"][:]).sum() == 0\n",
    "\n",
    "    x = hf[\"train data\"][:]\n",
    "    # assert (x - ctr_X[:train_num, :]).sum() == 0\n",
    "    print(f\"train data: {x.shape}\")\n",
    "    \n",
    "    x = hf[\"valid data\"][:]\n",
    "    # assert (x - ctr_X[train_num:train_num + valid_num, :]).sum() == 0\n",
    "    print(f\"valid data: {x.shape}\")\n",
    "    \n",
    "    x = hf[\"test data\"][:]\n",
    "    # assert (x - ctr_X[train_num + valid_num:, :]).sum() == 0\n",
    "    print(f\"test data: {x.shape}\")\n",
    "    \n",
    "    x = hf[\"train label\"][:]\n",
    "    # assert (x - ctr_Y[:train_num]).sum() == 0\n",
    "    print(f\"train label: {x.shape}\")\n",
    "    \n",
    "    x = hf[\"valid label\"][:]\n",
    "    # assert (x - ctr_Y[train_num:train_num + valid_num]).sum() == 0\n",
    "    print(f\"valid label: {x.shape}\")\n",
    "    \n",
    "    x = hf[\"test label\"][:]\n",
    "    # assert (x - ctr_Y[train_num + valid_num:]).sum() == 0\n",
    "    print(f\"test label: {x.shape}\")\n",
    "\n",
    "with open(os.path.join(target_dir, \"ctr.h5\"), \"rb\") as f:\n",
    "    md5_hash = hashlib.md5(f.read()).hexdigest()\n",
    "    print(\"ctr.h5\", md5_hash)\n",
    "    # assert md5_hash == \"6ee7a3247961653df021bb3514d1837f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: ensure each row from DataFrame and ctr is matched\n",
    "\n",
    "split_names = [\"train\", \"valid\", \"test\"]\n",
    "\n",
    "dataset = {split: pd.read_parquet(os.path.join(target_dir, f\"{split}.parquet.gz\")) for split in split_names}\n",
    "\n",
    "with h5py.File(os.path.join(target_dir, f\"ctr.h5\"), \"r\") as hf:\n",
    "    ctr_data = {\n",
    "        \"data\": {split: hf[f\"{split} data\"][:] for split in split_names},\n",
    "        \"label\": {split: hf[f\"{split} label\"][:] for split in split_names},\n",
    "    }\n",
    "\n",
    "for split in split_names:\n",
    "    for idx, row in tqdm(dataset[split].iterrows()):\n",
    "        for fi, field in enumerate(field_names):\n",
    "            if field not in [\"Movie ID\", \"User ID\"]:\n",
    "                pass\n",
    "                # assert feature_dict[field][row[field]] == ctr_data[\"data\"][split][idx, fi]\n",
    "            else:\n",
    "                pass\n",
    "                # assert row[field] == ctr_data[\"data\"][split][idx, fi]\n",
    "        # assert int(row[\"labels\"]) == ctr_data[\"label\"][split][idx]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
