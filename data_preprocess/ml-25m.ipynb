{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "from transformers import set_seed\n",
    "import hashlib\n",
    "import json\n",
    "import pickle as pkl\n",
    "import h5py\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "dataset = \"ml-25m\"\n",
    "root = f\"../data/{dataset_name}\"\n",
    "source_dir = os.path.join(root, \"raw_data\")\n",
    "target_dir = os.path.join(root, \"proc_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movie data\n",
    "\n",
    "movie_fields = [\"Movie ID\", \"Movie title\", \"Movie genre\"]\n",
    "df_movie = pd.read_csv(os.path.join(source_dir, \"movies.csv\"))\n",
    "df_movie = df_movie.rename(columns={'movieId': 'Movie ID', 'title': 'Movie title', 'genres': 'Movie genre'})\n",
    "\n",
    "df_movie['Movie genre'] = df_movie['Movie genre'].apply(lambda x: x.strip().split(\"|\")[0])\n",
    "print(f\"Total number of movies: {len(df_movie)}\")\n",
    "assert len(df_movie[\"Movie ID\"]) == len(set(df_movie[\"Movie ID\"]))\n",
    "\n",
    "df_movie.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating data\n",
    "\n",
    "rating_fields = [\"User ID\", \"Movie ID\", \"rating\", \"timestamp\", \"labels\"]\n",
    "df_ratings = pd.read_csv(os.path.join(source_dir, \"ratings.csv\"))\n",
    "df_ratings = df_ratings.rename(columns={'userId': 'User ID', 'movieId': 'Movie ID'})\n",
    "df_ratings[\"labels\"] = df_ratings[\"rating\"].apply(lambda x: int(x > 3))\n",
    "\n",
    "print(f\"Total number of ratings: {len(df_ratings)}\")\n",
    "df_ratings.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_ratings['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_movie/df_rating into df_data\n",
    "\n",
    "df_data = pd.merge(df_ratings, df_movie, on=[\"Movie ID\"], how=\"inner\")\n",
    "df_data.sort_values(by=[\"timestamp\", \"User ID\", \"Movie ID\"], inplace=True, kind=\"stable\")\n",
    "df_data = df_data.reset_index(drop=True)\n",
    "\n",
    "print(f\"{len(df_data)}\")\n",
    "df_data.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the feature dict for CTR data\n",
    "\n",
    "def add_to_dict(dict, feature):\n",
    "    if feature not in dict:\n",
    "        dict[feature] = len(dict)\n",
    "\n",
    "field_names = [\"User ID\", \"Movie ID\", \"Movie title\", \"Movie genre\"]\n",
    "feature_dict = {field : {} for field in field_names}\n",
    "\n",
    "for idx, row in tqdm(df_data.iterrows()):\n",
    "    for field in field_names:\n",
    "        add_to_dict(feature_dict[field], row[field])\n",
    "\n",
    "feature_count = [len(feature_dict[field]) for field in field_names]\n",
    "\n",
    "feature_offset = [0]\n",
    "for c in feature_count[:-1]:\n",
    "    feature_offset.append(feature_offset[-1] + c)\n",
    "\n",
    "for field in field_names:\n",
    "    print(field, len(feature_dict[field]))\n",
    "    assert len(feature_dict[field]) == len(set(list(df_data[field])))\n",
    "\n",
    "print(\"---------------------------------------------------------------\")\n",
    "for f, fc, fo in zip(field_names, feature_count, feature_offset):\n",
    "    print(f, fc, fo)\n",
    "print(\"---------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-encode `User ID` and `Movie ID` in `df_data`\n",
    "\n",
    "re_encode_fields = [\"User ID\", \"Movie ID\"]\n",
    "for field in re_encode_fields:\n",
    "    df_data[field] = df_data[field].apply(lambda x: feature_dict[field][str(x)])\n",
    "\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect user history\n",
    "\n",
    "user_history_dict = {\n",
    "    \"ID\": {k: [] for k in set(df_data[\"User ID\"])},\n",
    "    \"rating\": {k: [] for k in set(df_data[\"User ID\"])},\n",
    "}\n",
    "history_column = {\n",
    "    \"ID\": [],\n",
    "    \"rating\": [],\n",
    "}\n",
    "movie_id_to_title = {}\n",
    "\n",
    "for idx, row in tqdm(df_data.iterrows()):\n",
    "    user_id, movie_id, rating, title = row[\"User ID\"], row[\"Movie ID\"], row[\"rating\"], row[\"Movie title\"]\n",
    "    history_column[\"ID\"].append(user_history_dict[\"ID\"][user_id].copy())\n",
    "    history_column[\"rating\"].append(user_history_dict[\"rating\"][user_id].copy())\n",
    "    user_history_dict[\"ID\"][user_id].append(movie_id)\n",
    "    user_history_dict[\"rating\"][user_id].append(rating)\n",
    "    if movie_id not in movie_id_to_title:\n",
    "        movie_id_to_title[movie_id] = title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop data sample with history length that is less than 5.\n",
    "\n",
    "df_data[\"history ID\"] = history_column[\"ID\"]\n",
    "df_data[\"history rating\"] = history_column[\"rating\"]\n",
    "\n",
    "df_data = df_data[df_data[\"history ID\"].apply(lambda x: len(x)) >= 5].reset_index(drop=True)\n",
    "\n",
    "history_column[\"ID\"] = [x for x in history_column[\"ID\"] if len(x) >= 5]\n",
    "history_column[\"rating\"] = [x for x in history_column[\"rating\"] if len(x) >= 5]\n",
    "history_column[\"hist length\"] = [len(x) for x in history_column[\"rating\"]]\n",
    "\n",
    "for idx, row in tqdm(df_data.iterrows()):\n",
    "    assert row[\"history ID\"] == history_column[\"ID\"][idx]\n",
    "    assert row[\"history rating\"] == history_column[\"rating\"][idx]\n",
    "    assert len(row[\"history rating\"]) == history_column[\"hist length\"][idx]\n",
    "\n",
    "print(f\"Number of data samples: {len(df_data)}\")\n",
    "print(f\"Average history length: {df_data['history ID'].apply(lambda x: len(x)).mean()}\")\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split & save user history sequence\n",
    "\n",
    "train_num = int(0.8 * len(df_data))\n",
    "valid_num = int(0.1 * len(df_data))\n",
    "test_num = len(df_data) - train_num - valid_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train/valid/test in parquet format\n",
    "\n",
    "df_train = df_data[:train_num].reset_index(drop=True)\n",
    "df_valid = df_data[train_num:train_num + valid_num].reset_index(drop=True)\n",
    "df_test = df_data[train_num + valid_num:].reset_index(drop=True)\n",
    "\n",
    "\n",
    "assert len(df_train) == train_num\n",
    "assert len(df_valid) == valid_num\n",
    "assert len(df_test) == test_num\n",
    "\n",
    "print(f\"Train num: {len(df_train)}\")\n",
    "print(f\"Valid num: {len(df_valid)}\")\n",
    "print(f\"Test num: {len(df_test)}\")\n",
    "\n",
    "df_train.to_parquet(os.path.join(target_dir, \"train.parquet.gz\"), compression=\"gzip\")\n",
    "df_valid.to_parquet(os.path.join(target_dir, \"valid.parquet.gz\"), compression=\"gzip\")\n",
    "# df_test.to_parquet(os.path.join(target_dir, \"test.parquet.gz\"), compression=\"gzip\")\n",
    "\n",
    "\n",
    "df_test_sampled = df_test.sample(n=70000, random_state=42)\n",
    "test_indice = list(df_test_sampled.index)\n",
    "df_test_sampled = df_test_sampled.reset_index(drop=True)\n",
    "df_test_sampled.to_parquet(os.path.join(target_dir, \"test.parquet.gz\"), compression=\"gzip\")\n",
    "\n",
    "df_train_sampled = df_train.sample(n=90000, random_state=42)\n",
    "df_train_sampled = df_train_sampled.reset_index(drop=True)\n",
    "df_train_sampled.to_parquet(os.path.join(target_dir, \"train_sampled.parquet.gz\"), compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_seq = {\n",
    "    \"history ID\": {\n",
    "        \"train\": history_column[\"ID\"][:train_num],\n",
    "        \"valid\": history_column[\"ID\"][train_num:train_num + valid_num],\n",
    "        \"test\": history_column[\"ID\"][test_indice],\n",
    "    },\n",
    "    \"history rating\": {\n",
    "        \"train\": history_column[\"rating\"][:train_num],\n",
    "        \"valid\": history_column[\"rating\"][train_num:train_num + valid_num],\n",
    "        \"test\": history_column[\"rating\"][test_indice],\n",
    "    },\n",
    "    \"history length\": {\n",
    "        \"train\": history_column[\"hist length\"][:train_num],\n",
    "        \"valid\": history_column[\"hist length\"][train_num:train_num + valid_num],\n",
    "        \"test\": history_column[\"hist length\"][test_indice],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "json.dump(user_seq, open(os.path.join(target_dir, \"user_seq.json\"), \"w\"), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = {\n",
    "    \"field_names\": field_names, # list of field names\n",
    "    \"feature_count\": feature_count, # list of field feature counts\n",
    "    \"feature_dict\": feature_dict,   # {field:{feat: idx from 0 to field feat counts}}\n",
    "    \"feature_offset\": feature_offset, # list [0, ...]\n",
    "    \"movie_id_to_title\": movie_id_to_title,\n",
    "    \"num_ratings\": len(set(df_data['rating'])),\n",
    "}\n",
    "\n",
    "json.dump(meta_data, open(os.path.join(target_dir, \"ctr-meta.json\"), \"w\"), ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_id_to_ctr_id = feature_dict['Movie ID']\n",
    "df_movie.set_index('Movie ID', inplace=True)\n",
    "ctr_id_to_movie_id = {int(ctr_id): int(movie_id) for movie_id, ctr_id in movie_id_to_ctr_id.items()}\n",
    "movie_detail = []\n",
    "for cur_id in sorted(ctr_id_to_movie_id.keys()):\n",
    "    cur_movie_id = ctr_id_to_movie_id[cur_id]\n",
    "    row_id = cur_movie_id\n",
    "    row = df_movie.loc[row_id]\n",
    "    movie_detail.append(row.tolist())\n",
    "movie_detail = pd.DataFrame(movie_detail, columns=['Movie title', 'Movie genre'])\n",
    "movie_detail.to_parquet(os.path.join(target_dir, 'ml_25m_movie_detail.parquet.gz'), compression=\"gzip\")\n",
    "movie_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert df_data to CTR data via feature_dict\n",
    "\n",
    "ctr_X, ctr_Y = [], []\n",
    "for idx, row in tqdm(df_data.iterrows()):\n",
    "    ctr_X.append([feature_dict[field][row[field]] if field not in [\"Movie ID\", \"User ID\"] else row[field] for field in field_names])\n",
    "    ctr_Y.append(int(row[\"labels\"]))\n",
    "\n",
    "ctr_X = np.array(ctr_X)\n",
    "ctr_Y = np.array(ctr_Y)\n",
    "print(\"ctr_X\", ctr_X.shape)\n",
    "print(\"ctr_Y\", ctr_Y.shape)\n",
    "feature_count_np = np.array(feature_count).reshape(1, -1)\n",
    "assert (ctr_X - feature_count_np <= 0).sum() == ctr_X.shape[0] * ctr_X.shape[1]\n",
    "assert (ctr_Y == 0).sum() + (ctr_Y == 1).sum() == ctr_Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate the user sequence up to 30, i.e., 5 <= length <= 30.\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "user_seq_trunc = {\n",
    "    \"history ID\": {}, \n",
    "    \"history rating\": {}, \n",
    "    \"history mask\": {}, \n",
    "}\n",
    "for hist_name in user_seq:\n",
    "    for split in user_seq[hist_name]:\n",
    "        if hist_name != \"history length\":\n",
    "            user_seq_trunc[hist_name][split] = pad_sequence(\n",
    "                [torch.tensor(x[-30:]) for x in user_seq[hist_name][split]], \n",
    "                batch_first=True, \n",
    "            )\n",
    "        else:\n",
    "            user_seq_trunc[\"history mask\"][split] = pad_sequence(\n",
    "                [torch.ones(min(x, 30)) for x in user_seq[hist_name][split]], \n",
    "                batch_first=True, \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CTR data & truncated user sequence into one .h5 file\n",
    "\n",
    "with h5py.File(os.path.join(target_dir, f\"ctr.h5\"), \"w\") as hf:\n",
    "    hf.create_dataset(\"train data\", data=ctr_X[:train_num, :])\n",
    "    hf.create_dataset(\"valid data\", data=ctr_X[train_num:train_num + valid_num, :])\n",
    "    hf.create_dataset(\"test data\", data=ctr_X[test_indice, :])\n",
    "    hf.create_dataset(\"train label\", data=ctr_Y[:train_num])\n",
    "    hf.create_dataset(\"valid label\", data=ctr_Y[train_num:train_num + valid_num])\n",
    "    hf.create_dataset(\"test label\", data=ctr_Y[test_indice])\n",
    "    for hist_name in user_seq_trunc:\n",
    "        for split in user_seq_trunc[hist_name]:\n",
    "            hf.create_dataset(f\"{split} {hist_name}\", data=user_seq_trunc[hist_name][split])\n",
    "\n",
    "\n",
    "with h5py.File(os.path.join(target_dir, f\"ctr.h5\"), \"r\") as hf:\n",
    "    for hist_name in user_seq_trunc:\n",
    "        for split in user_seq_trunc[hist_name]:\n",
    "            assert (user_seq_trunc[hist_name][split] - hf[f\"{split} {hist_name}\"][:]).sum() == 0\n",
    "    \n",
    "    x = hf[\"train data\"][:]\n",
    "    assert (x - ctr_X[:train_num, :]).sum() == 0\n",
    "    print(f\"train data: {x.shape}\")\n",
    "    \n",
    "    x = hf[\"valid data\"][:]\n",
    "    assert (x - ctr_X[train_num:train_num + valid_num, :]).sum() == 0\n",
    "    print(f\"valid data: {x.shape}\")\n",
    "    \n",
    "    x = hf[\"test data\"][:]\n",
    "    assert (x - ctr_X[test_indice, :]).sum() == 0\n",
    "    print(f\"test data: {x.shape}\")\n",
    "    \n",
    "    x = hf[\"train label\"][:]\n",
    "    assert (x - ctr_Y[:train_num]).sum() == 0\n",
    "    print(f\"train label: {x.shape}\")\n",
    "    \n",
    "    x = hf[\"valid label\"][:]\n",
    "    assert (x - ctr_Y[train_num:train_num + valid_num]).sum() == 0\n",
    "    print(f\"valid label: {x.shape}\")\n",
    "    \n",
    "    x = hf[\"test label\"][:]\n",
    "    assert (x - ctr_Y[test_indice]).sum() == 0\n",
    "    print(f\"test label: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: ensure each row from DataFrame and ctr is matched\n",
    "\n",
    "split_names = [\"train\", \"test\"]\n",
    "\n",
    "dataset = {split: pd.read_parquet(os.path.join(target_dir, f\"{split}.parquet.gz\")) for split in split_names}\n",
    "\n",
    "with h5py.File(os.path.join(target_dir, f\"ctr.h5\"), \"r\") as hf:\n",
    "    ctr_data = {\n",
    "        \"data\": {split: hf[f\"{split} data\"][:] for split in split_names},\n",
    "        \"label\": {split: hf[f\"{split} label\"][:] for split in split_names},\n",
    "    }\n",
    "\n",
    "for split in split_names:\n",
    "    for idx, row in tqdm(dataset[split].iterrows()):\n",
    "        for fi, field in enumerate(field_names):\n",
    "            if field not in [\"Movie ID\", \"User ID\"]:\n",
    "                assert feature_dict[field][row[field]] == ctr_data[\"data\"][split][idx, fi]\n",
    "            else:\n",
    "                print(row[field])\n",
    "                print(ctr_data[\"data\"][split][idx, fi])\n",
    "                assert row[field] == ctr_data[\"data\"][split][idx, fi]\n",
    "        assert int(row[\"labels\"]) == ctr_data[\"label\"][split][idx]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huawei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
