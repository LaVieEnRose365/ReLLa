{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "from transformers import set_seed\n",
    "import hashlib\n",
    "import json\n",
    "import pickle as pkl\n",
    "import h5py\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "dataset_name = \"BookCrossing\"\n",
    "root = f\"../data/{dataset_name}\"\n",
    "source_dir = os.path.join(root, \"raw_data\")\n",
    "target_dir = os.path.join(root, \"proc_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import ascii_letters, digits\n",
    "\n",
    "def character_check(item, special_letters=\"\"):\n",
    "    for letter in str(item):\n",
    "        if letter not in ascii_letters + digits + special_letters:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_users 7cc0a4c37b494183233b02fcead5ea1b\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User ID</th>\n",
       "      <th>Location</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>USA</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>USA</td>\n",
       "      <td>18-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Russia</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>under 18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>USA</td>\n",
       "      <td>60+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>USA</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Canada</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>USA</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Spain</td>\n",
       "      <td>25-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Australia</td>\n",
       "      <td>under 18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>USA</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Spain</td>\n",
       "      <td>25-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>USA</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Canada</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>USA</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>USA</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>25-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>unknown</td>\n",
       "      <td>under 18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>USA</td>\n",
       "      <td>18-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User ID        Location       Age\n",
       "0        1             USA   unknown\n",
       "1        2             USA     18-24\n",
       "2        3          Russia   unknown\n",
       "3        4        Portugal  under 18\n",
       "4        5  United Kingdom   unknown\n",
       "5        6             USA       60+\n",
       "6        7             USA   unknown\n",
       "7        8          Canada   unknown\n",
       "8        9             USA   unknown\n",
       "9       10           Spain     25-29\n",
       "10      11       Australia  under 18\n",
       "11      12             USA   unknown\n",
       "12      13           Spain     25-29\n",
       "13      14             USA   unknown\n",
       "14      15          Canada   unknown\n",
       "15      16             USA   unknown\n",
       "16      17             USA   unknown\n",
       "17      18          Brazil     25-29\n",
       "18      19         unknown  under 18\n",
       "19      20             USA     18-24"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read user info\n",
    "\n",
    "user_fields = [\"User ID\", \"Location\", \"Age\"]\n",
    "pattern = re.compile(r'NULL|\".*?(?<!\\\\)\"', re.S)\n",
    "with open(os.path.join(source_dir, \"BX-Users.csv\"), 'r', encoding='cp1252') as f:\n",
    "    content = pattern.findall(f.read())\n",
    "    content = [s[1:-1] if s != 'NULL' else None for s in content]\n",
    "    processed_list = list(np.array(content).reshape((-1, 3)))\n",
    "    processed_list.pop(0)\n",
    "    df_users = pd.DataFrame(processed_list, columns=user_fields)\n",
    "\n",
    "# There are messy info/code (or totally empty) in the `Location` field, we only use the country instead.\n",
    "# E.g., ['&#37073;&#24030;&#26159;, &#20013;&#22269;&#27827;&#21335;&#30465;&#37073;&#24030;&#24066;, china', \n",
    "#        'philippine science high school - cmc, mcc main stadium, sagadan, tubod, lanao del norte, philippines', \n",
    "#        '6.a.4.a.6.a`4.a, 6.a.4.a.6.a`4.a.6.a`4.a.6.a.4.a.6.a`4.aoe6.a`4.a -- 6.a.4.a.6.a`4.aoe6.a`4.a ã??, ä¸\\xadå?½']\n",
    "def convert_location_to_country(x):\n",
    "    x = x.split(', ')[-1].strip().title().replace(\"!\", \"\").strip()\n",
    "    if x.lower() in [\"usa\", \"us\", \"u s\", \"u s a\"]:\n",
    "        x = \"USA\"\n",
    "    if x.lower() in [\"uk\", \"u k\"]:\n",
    "        x = \"UK\"\n",
    "    while len(x) > 0 and x[-1] in [\",\", \".\"]:\n",
    "        x = x[:-1]\n",
    "    while len(x) > 0 and x[0] in [\",\", \".\"]:\n",
    "        x = x[1:]\n",
    "    if \"U.S\" in x.upper() and x != \"U.S. Virgin Islands\":\n",
    "        x = \"USA\"\n",
    "    if x in [\"San José\", \"San Josï¿½\"]:\n",
    "        x = \"USA\"\n",
    "    if x in [\"España\", \"Castilla-León\", \"Espaã±A\", \"Cataluña\", \"Mérida\", \"Álava\", \"Málaga\", \"A Coruña\", \"Barcelonès\", \"Berguedà\",\n",
    "              \"Espaï¿½A\", \"Castilla-Leï¿½N\", \"A Coruï¿½A\", \"Cataluï¿½A\", \"Barcelonï¿½S\", \"Ï¿½Lava\", \"Mï¿½Rida\", \"Berguedï¿½\", \"Mï¿½Laga\"] or \"spain\" in x.lower():\n",
    "        x = \"Spain\"\n",
    "    if x in [\"L`Italia\"]:\n",
    "        x = \"Italy\"\n",
    "    if x in [\"Baden-Württemberg\", \"Bademn Würtemberg\", \"Baden-Wï¿½Rttemberg\", \"Bademn Wï¿½Rtemberg\"]:\n",
    "        x = \"German\"\n",
    "    if x in [\"Cote D`Ivoire\", \"Côte D\", \"Cï¿½Te D\"]:\n",
    "        x = \"Ivory Coast\"\n",
    "    if x in [\"Oberösterreich\", \"Oberï¿½Sterreich\"]:\n",
    "        x = \"Austria\"\n",
    "    if x in [\"México\", \"Mï¿½Xico\"]:\n",
    "        x = \"Mexico\"\n",
    "    if x in [\"Türkiye\", \"Içel\", \"Tï¿½Rkiye\"]:\n",
    "        x = \"Turkey\"\n",
    "    if x in [\"L`Algérie\", \"Algérie\", \"Kärnten\", \"Kï¿½Rnten\", \"L`Algï¿½Rie\", \"Algï¿½Rie\"]:\n",
    "        x = \"Algeria\"\n",
    "    if \"Brasil\" in x:\n",
    "        x = \"Brazil\"\n",
    "    if x in [\"Rhône-Alpes\", \"Rhône Alpes\", \"Rhï¿½Ne-Alpes\", \"Rhï¿½Ne Alpes\"]:\n",
    "        x = \"France\"\n",
    "    if \"Greece\" in x:\n",
    "        x = \"Greece\"\n",
    "    if x in [\"Santarém\", \"Santarï¿½M\"]:\n",
    "        x = \"Portugal\"\n",
    "    if x in [\"Länsi-Suomen Lääni\", \"Lï¿½Nsi-Suomen Lï¿½Ï¿½Ni\"]:\n",
    "        x = \"Finland\"\n",
    "    if x in [\"V.Götaland\", \"Nyhamnsläge\", \"V.Gï¿½Taland\", \"Nyhamnslï¿½Ge\"]:\n",
    "        x = \"Sweden\"\n",
    "    if x in [\"Moçambique\", \"Moï¿½Ambique\"]:\n",
    "        x = \"Mozambique\"\n",
    "    if x in [\"Ix Región\", \"Ix Regiï¿½N\"]:\n",
    "        x = \"Chile\"\n",
    "    if x in [\"Maï¿½Opolskie\", \"Ma³Opolskie\"]:\n",
    "        x = \"Poland\"\n",
    "    if x in [\"Perï¿½\", \"Perãº\"]:\n",
    "        x = \"Peru\"\n",
    "    if x != \"China\" and (\"china\" in x.lower() or x == \"La Chine Éternelle\" or x == \"La Chine Ï¿½Ternelle\"):\n",
    "        x = \"China\"\n",
    "    if x == \"Ï¿½Ï¿½Ï¿½\":\n",
    "        x = \"China\"\n",
    "    if (x == \"\" or \\\n",
    "        x in [\"Öð¹Ú\", \"ºþäï\", \"We`Re Global\", \"Ï¿½Ï¿½Ï¿½Ï¿½\", \"Iï¿½El\"] or \\\n",
    "        len(x) == 1 or \\\n",
    "        \"N/A\" in x or \\\n",
    "        \"&#\" in x or \\\n",
    "        \"?\" in x or \\\n",
    "        \"@\" in x or \\\n",
    "        \"*\" in x):\n",
    "        x = \"unknown\"\n",
    "    return x\n",
    "df_users[\"Location\"] = df_users[\"Location\"].apply(convert_location_to_country)\n",
    "df_users[\"location_check\"] = df_users[\"Location\"].apply(lambda x: character_check(x, special_letters=\"- .&/()\"))\n",
    "\n",
    "assert len(df_users.loc[df_users[\"location_check\"] == 1, \"Location\"]) == 0\n",
    "\n",
    "# Nearly a half of the features in `Age` field are missing.\n",
    "def convert_age_to_bucket(x):\n",
    "    if x is None:\n",
    "        x = \"unknown\"\n",
    "    else:\n",
    "        x = int(x)\n",
    "        # There are out-of-range ages (e.g., < 5 or > 100).\n",
    "        if x < 5 or x > 100:\n",
    "            x = \"unknown\"\n",
    "        # Age discretization\n",
    "        elif x < 18:\n",
    "            x = \"under 18\"\n",
    "        elif 18 <= x < 25:\n",
    "            x = \"18-24\"\n",
    "        elif 25 <= x < 30:\n",
    "            x = \"25-29\"\n",
    "        elif 30 <= x < 35:\n",
    "            x = \"30-34\"\n",
    "        elif 35 <= x < 40:\n",
    "            x = \"35-39\"\n",
    "        elif 40 <= x < 45:\n",
    "            x = \"40-44\"\n",
    "        elif 45 <= x < 50:\n",
    "            x = \"45-49\"\n",
    "        elif 50 <= x < 55:\n",
    "            x = \"50-54\"\n",
    "        elif 55 <= x < 60:\n",
    "            x = \"55-59\"\n",
    "        else:\n",
    "            x = \"60+\"\n",
    "    return x\n",
    "df_users[\"Age\"] = df_users[\"Age\"].apply(convert_age_to_bucket)\n",
    "\n",
    "for field in user_fields:\n",
    "    for s in list(df_users[field]):\n",
    "        if field == \"User ID\":\n",
    "            assert 1 <= int(s) <= 278858\n",
    "        if field == \"Location\":\n",
    "            assert 2 <= len(s) <= 45\n",
    "        if field == \"Age\":\n",
    "            assert s in [\"unknown\", \"under 18\" ,\"18-24\", \"25-29\", \"30-34\", \"35-39\", \"40-44\", \"45-49\", \"50-54\", \"55-59\", \"60+\"]\n",
    "\n",
    "df_users = df_users[user_fields]\n",
    "\n",
    "md5_hash = hashlib.md5(json.dumps(df_users.values.tolist(), sort_keys=True).encode('utf-8')).hexdigest()\n",
    "print(\"df_users\", md5_hash)\n",
    "df_users.head(20)\n",
    "assert md5_hash == \"7cc0a4c37b494183233b02fcead5ea1b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_books bd070d039ad5d48228c0f256e61f381e\n"
     ]
    }
   ],
   "source": [
    "# Read book info\n",
    "\n",
    "book_fields = [\"ISBN\", \"Book title\", \"Author\", \"Publication year\", \"Publisher\"]\n",
    "pattern = re.compile(r'(?<=\");(?=\")')\n",
    "processed_list = []\n",
    "with open(os.path.join(source_dir, \"BX-Books.csv\"), 'r', encoding='cp1252') as f:\n",
    "    for line in f.readlines():\n",
    "        split_line = pattern.split(line.strip())\n",
    "        split_line = [item[1:-1].strip('\\t') for item in split_line][:-3] # The last three image URLs are not needed.\n",
    "        processed_list.append(split_line)\n",
    "    processed_list.pop(0)\n",
    "    df_books = pd.DataFrame(processed_list, columns=book_fields)\n",
    "\n",
    "# ISBN should only contain letters and digits.\n",
    "df_books['ISBN_check'] = df_books['ISBN'].apply(lambda x: character_check(x))\n",
    "df_books = df_books[df_books['ISBN_check'] == 0]\n",
    "\n",
    "# There are invalid publication years, i.e., \"0\"\n",
    "def convert_publication_year(x):\n",
    "    x = x if len(x) == 4 else \"unknown\"\n",
    "    return x\n",
    "df_books[\"Publication year\"] = df_books[\"Publication year\"].apply(convert_publication_year)\n",
    "\n",
    "df_books[\"Publisher\"] = df_books[\"Publisher\"].apply(lambda x: x if x.lower() != \"n/a\" else \"unknown\")\n",
    "df_books[\"Author\"] = df_books[\"Author\"].apply(lambda x: x if x.lower() != \"n/a\" else \"unknown\")\n",
    "\n",
    "for field in book_fields:\n",
    "    for s in list(df_books[field]):\n",
    "        if field == \"ISBN\":\n",
    "            assert len(s) == 10\n",
    "        if field == \"Book title\":\n",
    "            assert 1 <= len(s) <= 256\n",
    "        if field == \"Author\":\n",
    "            assert 1 <= len(s) <= 143\n",
    "        if field == \"Publication year\":\n",
    "            assert s == \"unknown\" or len(s) == 4\n",
    "        if field == \"Publisher\":\n",
    "            assert 1 <= len(s) <= 134\n",
    "\n",
    "df_books = df_books[book_fields]\n",
    "print(df_books.head())\n",
    "print('---------------------------------------------------------------')\n",
    "print(df_books.info())\n",
    "print('---------------------------------------------------------------')\n",
    "print(df_books.describe())\n",
    "print('---------------------------------------------------------------')\n",
    "md5_hash = hashlib.md5(json.dumps(df_books.values.tolist(), sort_keys=True).encode('utf-8')).hexdigest()\n",
    "print(\"df_books\", md5_hash)\n",
    "assert md5_hash == \"bd070d039ad5d48228c0f256e61f381e\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User ID 278858\n",
      "Location 922\n",
      "Age 11\n",
      "ISBN 271375\n",
      "Book title 242152\n",
      "Author 102027\n",
      "Publication year 116\n",
      "Publisher 16807\n"
     ]
    }
   ],
   "source": [
    "# Encode features\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def add_to_dict(dict, feature):\n",
    "    if feature not in dict:\n",
    "        dict[feature] = len(dict)\n",
    "\n",
    "feature_dict = {field : {} for field in user_fields + book_fields}\n",
    "user_dict = {}\n",
    "book_dict = {}\n",
    "\n",
    "for idx, row in df_users.iterrows():\n",
    "    if row[\"User ID\"] not in user_dict:\n",
    "        user_dict[row[\"User ID\"]] = [row[\"Location\"], row[\"Age\"]]\n",
    "    for field in user_fields:\n",
    "        add_to_dict(feature_dict[field], row[field])\n",
    "\n",
    "for idx, row in df_books.iterrows():\n",
    "    if row[\"ISBN\"] not in book_dict:\n",
    "        book_dict[row[\"ISBN\"]] = [row[\"Book title\"], row[\"Author\"], row[\"Publication year\"], row[\"Publisher\"]]\n",
    "    for field in book_fields:\n",
    "        add_to_dict(feature_dict[field], row[field])\n",
    "\n",
    "feature_count = [len(feature_dict[field]) for field in user_fields + book_fields]\n",
    "\n",
    "for field in user_fields:\n",
    "    print(field, len(feature_dict[field]))\n",
    "    assert len(feature_dict[field]) == len(set(list(df_users[field])))\n",
    "\n",
    "for field in book_fields:\n",
    "    print(field, len(feature_dict[field]))\n",
    "    assert len(feature_dict[field]) == len(set(list(df_books[field])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump(book_dict, open(os.path.join(target_dir, \"book_dict.json\"), \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read ratings\n",
    "\n",
    "processed_list = []\n",
    "with open(os.path.join(source_dir, \"BX-Book-Ratings.csv\"), 'r', encoding='cp1252') as f:\n",
    "    for line in f.readlines():\n",
    "        split_line = line.strip().split(';')\n",
    "        split_line = [item[1:-1] for item in split_line]\n",
    "        processed_list.append(split_line)\n",
    "    column_list = processed_list[0]\n",
    "    processed_list.pop(0)\n",
    "\n",
    "\n",
    "user_hist, hist_rating, labels = {}, {}, {}\n",
    "for user, isbn, rating in processed_list:\n",
    "    if user in feature_dict[\"User ID\"] and isbn in feature_dict[\"ISBN\"]:\n",
    "        if user not in user_hist:\n",
    "            user_hist[user] = []\n",
    "            hist_rating[user] = []\n",
    "            labels[user] = []\n",
    "        user_hist[user].append(isbn)\n",
    "        hist_rating[user].append(int(rating))\n",
    "        labels[user].append(int(int(rating) >= 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users deleted: 74392\n"
     ]
    }
   ],
   "source": [
    "# Generate and shuffle data samples in DataFrame format\n",
    "\n",
    "data_list = []\n",
    "\n",
    "# filter users who rated no more than 5 books\n",
    "user_del = []\n",
    "for user, hist in user_hist.items():\n",
    "    cnt = 0\n",
    "    if len(hist) <= 5:\n",
    "        user_del.append(user)\n",
    "\n",
    "print(\"Number of users deleted:\", len(user_del))\n",
    "\n",
    "for user in user_del:\n",
    "    del user_hist[user]\n",
    "    del hist_rating[user]\n",
    "    del labels[user]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17714\n"
     ]
    }
   ],
   "source": [
    "for user in user_hist.keys():\n",
    "    zipped_data = list(zip(user_hist[user], hist_rating[user], labels[user]))\n",
    "    set_seed(42)\n",
    "    random.shuffle(zipped_data)\n",
    "    user_hist[user], hist_rating[user], labels[user] = map(list, zip(*zipped_data))\n",
    "    isbn = user_hist[user][-1]\n",
    "    data_sample = copy.deepcopy([user] + user_dict[user] + [isbn] + book_dict[isbn] +\n",
    "                                    [user_hist[user][:-1]] + [hist_rating[user][:-1]] + [labels[user][-1]] + [hist_rating[user][-1]])\n",
    "    data_list.append(data_sample)\n",
    "\n",
    "print(len(data_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "count = {}\n",
    "for user, hist in user_hist.items():\n",
    "    cnt += len(hist) - 1\n",
    "    if len(hist)-1 not in count:\n",
    "        count[len(hist)-1] = 0\n",
    "    count[len(hist)-1] += 1\n",
    "print(\"avg\", cnt/10784)\n",
    "\n",
    "print(\"*\"*50)\n",
    "print(\"Hist lens / Number of users\")\n",
    "for cnt in sorted(count.keys()):\n",
    "    print(cnt, count[cnt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 17714\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User ID</th>\n",
       "      <th>Location</th>\n",
       "      <th>Age</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Publication year</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>user_hist</th>\n",
       "      <th>hist_rating</th>\n",
       "      <th>labels</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>226267</td>\n",
       "      <td>USA</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0743417682</td>\n",
       "      <td>From a Buick 8</td>\n",
       "      <td>Stephen King</td>\n",
       "      <td>2003</td>\n",
       "      <td>Pocket Books</td>\n",
       "      <td>[051513287X, 0515131229, 0312982518, 067103818...</td>\n",
       "      <td>[6, 0, 9, 6, 10, 6, 7, 9, 8, 7, 10]</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57863</td>\n",
       "      <td>USA</td>\n",
       "      <td>40-44</td>\n",
       "      <td>0812558626</td>\n",
       "      <td>Briar Rose</td>\n",
       "      <td>Jane Yolen</td>\n",
       "      <td>1993</td>\n",
       "      <td>Tor Books</td>\n",
       "      <td>[0316955124, 0380727501, 0609809547, 038071089...</td>\n",
       "      <td>[0, 0, 0, 0, 4, 8]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>95636</td>\n",
       "      <td>USA</td>\n",
       "      <td>30-34</td>\n",
       "      <td>1551669005</td>\n",
       "      <td>Parting Gifts</td>\n",
       "      <td>Charlotte Allen</td>\n",
       "      <td>2002</td>\n",
       "      <td>Mira</td>\n",
       "      <td>[0515125490, 0385264267, 0440240913, 067102136...</td>\n",
       "      <td>[8, 0, 0, 8, 7]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51481</td>\n",
       "      <td>Germany</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0373034709</td>\n",
       "      <td>To Marry A Stranger  (Enchanted Brides) (Harle...</td>\n",
       "      <td>Renee Roszel</td>\n",
       "      <td>1997</td>\n",
       "      <td>Harlequin</td>\n",
       "      <td>[3404131215, 0373029373, 037311513X, 037316171...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>190147</td>\n",
       "      <td>Italy</td>\n",
       "      <td>unknown</td>\n",
       "      <td>8807700948</td>\n",
       "      <td>L'amico del pazzo a altri racconti (I Canguri/...</td>\n",
       "      <td>Marco Drago</td>\n",
       "      <td>1998</td>\n",
       "      <td>Feltrinelli</td>\n",
       "      <td>[0375700498, 8408002511, 8807102005, 089471796...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>229041</td>\n",
       "      <td>USA</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0609803697</td>\n",
       "      <td>Freedom from Asthma: The Revolutionary 5-Day T...</td>\n",
       "      <td>Alexander Stalmatski</td>\n",
       "      <td>1999</td>\n",
       "      <td>Three Rivers Press (CA)</td>\n",
       "      <td>[0451513355, 0449202917, 0393321096, 055321041...</td>\n",
       "      <td>[9, 10, 9, 5, 6, 7, 0, 9, 8, 9, 10, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>272961</td>\n",
       "      <td>USA</td>\n",
       "      <td>50-54</td>\n",
       "      <td>0312978901</td>\n",
       "      <td>Born Evil (Claremont Studies in the Philosophy...</td>\n",
       "      <td>Adrian Havill</td>\n",
       "      <td>2001</td>\n",
       "      <td>St. Martin's True Crime Classics</td>\n",
       "      <td>[0671027387, 0345455207, 0312995423, 081251528...</td>\n",
       "      <td>[7, 9, 6, 0, 0, 6, 0, 6, 9]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>140997</td>\n",
       "      <td>Germany</td>\n",
       "      <td>40-44</td>\n",
       "      <td>3596220742</td>\n",
       "      <td>Die wunderbaren Jahre.</td>\n",
       "      <td>Reiner Kunze</td>\n",
       "      <td>1978</td>\n",
       "      <td>Fischer (Tb.), Frankfurt</td>\n",
       "      <td>[3499134705, 3442420024, 3492226965, 351836987...</td>\n",
       "      <td>[6, 0, 7, 0, 7]</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>202260</td>\n",
       "      <td>Spain</td>\n",
       "      <td>unknown</td>\n",
       "      <td>8479534850</td>\n",
       "      <td>Sabiduria Interior</td>\n",
       "      <td>Louise L. Hay</td>\n",
       "      <td>2002</td>\n",
       "      <td>Ediciones Urano</td>\n",
       "      <td>[840208348X, 2253012726, 0805029516, 058253701...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>39147</td>\n",
       "      <td>USA</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0821768859</td>\n",
       "      <td>Champion of the Heart (Zebra Historical Romanc...</td>\n",
       "      <td>Laurel O'Donnell</td>\n",
       "      <td>2001</td>\n",
       "      <td>Kensington Publishing Corporation</td>\n",
       "      <td>[0425185222, 0373710992, 037371162X, 055325800...</td>\n",
       "      <td>[0, 6, 8, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>153279</td>\n",
       "      <td>USA</td>\n",
       "      <td>50-54</td>\n",
       "      <td>0312288778</td>\n",
       "      <td>Nowhere to Run</td>\n",
       "      <td>Mary Jane Clark</td>\n",
       "      <td>2003</td>\n",
       "      <td>St. Martin's Press</td>\n",
       "      <td>[0385510187, 0786868716, 1551667363, 067176074...</td>\n",
       "      <td>[0, 8, 8, 6, 9, 0, 8, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>89179</td>\n",
       "      <td>Italy</td>\n",
       "      <td>30-34</td>\n",
       "      <td>8807813025</td>\n",
       "      <td>Novocento, Un Monologo</td>\n",
       "      <td>Alessandro Baricco</td>\n",
       "      <td>2003</td>\n",
       "      <td>Distribooks Inc</td>\n",
       "      <td>[8408022393, 0553573926, 0446605891, 880614304...</td>\n",
       "      <td>[0, 0, 0, 9, 7, 0, 0, 0, 0, 0, 0, 0, 8, 7, 0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>70940</td>\n",
       "      <td>USA</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0893012505</td>\n",
       "      <td>Tough Trip Through Paradise, 1878-1879</td>\n",
       "      <td>Andrew Garcia</td>\n",
       "      <td>2001</td>\n",
       "      <td>University of Idaho Press</td>\n",
       "      <td>[0375407251, 0060175966, 034541005X, 044022103...</td>\n",
       "      <td>[8, 8, 9, 0, 8]</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>121594</td>\n",
       "      <td>USA</td>\n",
       "      <td>60+</td>\n",
       "      <td>0195101405</td>\n",
       "      <td>Adventures of Huckleberry Finn (Oxford Mark Tw...</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>1996</td>\n",
       "      <td>Oxford University Press</td>\n",
       "      <td>[0451179803, 0826412963, 0061031992, 034540803...</td>\n",
       "      <td>[0, 10, 8, 0, 0, 10, 8, 10, 10, 0, 0, 0, 8, 8,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>23699</td>\n",
       "      <td>USA</td>\n",
       "      <td>25-29</td>\n",
       "      <td>0316777730</td>\n",
       "      <td>Naked</td>\n",
       "      <td>David Sedaris</td>\n",
       "      <td>1998</td>\n",
       "      <td>Back Bay Books</td>\n",
       "      <td>[0345339703, 034543479X, 0316284955, 068484267...</td>\n",
       "      <td>[10, 10, 0, 0, 10, 9, 5, 7, 10, 0, 8, 10, 10, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>226809</td>\n",
       "      <td>USA</td>\n",
       "      <td>60+</td>\n",
       "      <td>0553382411</td>\n",
       "      <td>The Elegant Gathering of White Snows</td>\n",
       "      <td>KRIS RADISH</td>\n",
       "      <td>2003</td>\n",
       "      <td>Bantam</td>\n",
       "      <td>[0330412140, 0060927968, 0156027321, 037540632...</td>\n",
       "      <td>[0, 0, 10, 0, 9]</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>126733</td>\n",
       "      <td>USA</td>\n",
       "      <td>under 18</td>\n",
       "      <td>0688155952</td>\n",
       "      <td>McBroom's Wonderful One-Acre Farm: Three Tall ...</td>\n",
       "      <td>Sid Fleischman</td>\n",
       "      <td>1997</td>\n",
       "      <td>HarperTrophy</td>\n",
       "      <td>[0553157248, 0394800060, 0517187868, 067988468...</td>\n",
       "      <td>[8, 0, 8, 6, 7]</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>68383</td>\n",
       "      <td>USA</td>\n",
       "      <td>35-39</td>\n",
       "      <td>0894716239</td>\n",
       "      <td>Mark Wilson's Complete Course in Magic</td>\n",
       "      <td>Mark Wilson</td>\n",
       "      <td>1991</td>\n",
       "      <td>Running Press Book Publishers</td>\n",
       "      <td>[0892815256, 0684815001, 0916441822, 080105806...</td>\n",
       "      <td>[0, 0, 0, 10, 10, 10, 10, 10, 10, 10, 10, 5, 8...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5585</td>\n",
       "      <td>USA</td>\n",
       "      <td>35-39</td>\n",
       "      <td>074343627X</td>\n",
       "      <td>Dreamcatcher</td>\n",
       "      <td>Stephen King</td>\n",
       "      <td>2001</td>\n",
       "      <td>Pocket</td>\n",
       "      <td>[0373250371, 0312291639, 0312983271, 045140888...</td>\n",
       "      <td>[0, 0, 5, 0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>120917</td>\n",
       "      <td>USA</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0679432477</td>\n",
       "      <td>Empire Falls</td>\n",
       "      <td>RICHARD RUSSO</td>\n",
       "      <td>2001</td>\n",
       "      <td>Knopf</td>\n",
       "      <td>[0385323867, 0385308477, 0385323832, 038533392...</td>\n",
       "      <td>[6, 8, 5, 8, 8]</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User ID Location       Age        ISBN  \\\n",
       "0   226267      USA   unknown  0743417682   \n",
       "1    57863      USA     40-44  0812558626   \n",
       "2    95636      USA     30-34  1551669005   \n",
       "3    51481  Germany   unknown  0373034709   \n",
       "4   190147    Italy   unknown  8807700948   \n",
       "5   229041      USA   unknown  0609803697   \n",
       "6   272961      USA     50-54  0312978901   \n",
       "7   140997  Germany     40-44  3596220742   \n",
       "8   202260    Spain   unknown  8479534850   \n",
       "9    39147      USA   unknown  0821768859   \n",
       "10  153279      USA     50-54  0312288778   \n",
       "11   89179    Italy     30-34  8807813025   \n",
       "12   70940      USA   unknown  0893012505   \n",
       "13  121594      USA       60+  0195101405   \n",
       "14   23699      USA     25-29  0316777730   \n",
       "15  226809      USA       60+  0553382411   \n",
       "16  126733      USA  under 18  0688155952   \n",
       "17   68383      USA     35-39  0894716239   \n",
       "18    5585      USA     35-39  074343627X   \n",
       "19  120917      USA   unknown  0679432477   \n",
       "\n",
       "                                           Book title                Author  \\\n",
       "0                                      From a Buick 8          Stephen King   \n",
       "1                                          Briar Rose            Jane Yolen   \n",
       "2                                       Parting Gifts       Charlotte Allen   \n",
       "3   To Marry A Stranger  (Enchanted Brides) (Harle...          Renee Roszel   \n",
       "4   L'amico del pazzo a altri racconti (I Canguri/...           Marco Drago   \n",
       "5   Freedom from Asthma: The Revolutionary 5-Day T...  Alexander Stalmatski   \n",
       "6   Born Evil (Claremont Studies in the Philosophy...         Adrian Havill   \n",
       "7                              Die wunderbaren Jahre.          Reiner Kunze   \n",
       "8                                  Sabiduria Interior         Louise L. Hay   \n",
       "9   Champion of the Heart (Zebra Historical Romanc...      Laurel O'Donnell   \n",
       "10                                     Nowhere to Run       Mary Jane Clark   \n",
       "11                             Novocento, Un Monologo    Alessandro Baricco   \n",
       "12             Tough Trip Through Paradise, 1878-1879         Andrew Garcia   \n",
       "13  Adventures of Huckleberry Finn (Oxford Mark Tw...            Mark Twain   \n",
       "14                                              Naked         David Sedaris   \n",
       "15               The Elegant Gathering of White Snows           KRIS RADISH   \n",
       "16  McBroom's Wonderful One-Acre Farm: Three Tall ...        Sid Fleischman   \n",
       "17             Mark Wilson's Complete Course in Magic           Mark Wilson   \n",
       "18                                       Dreamcatcher          Stephen King   \n",
       "19                                       Empire Falls         RICHARD RUSSO   \n",
       "\n",
       "   Publication year                          Publisher  \\\n",
       "0              2003                       Pocket Books   \n",
       "1              1993                          Tor Books   \n",
       "2              2002                               Mira   \n",
       "3              1997                          Harlequin   \n",
       "4              1998                        Feltrinelli   \n",
       "5              1999            Three Rivers Press (CA)   \n",
       "6              2001   St. Martin's True Crime Classics   \n",
       "7              1978           Fischer (Tb.), Frankfurt   \n",
       "8              2002                    Ediciones Urano   \n",
       "9              2001  Kensington Publishing Corporation   \n",
       "10             2003                 St. Martin's Press   \n",
       "11             2003                    Distribooks Inc   \n",
       "12             2001          University of Idaho Press   \n",
       "13             1996            Oxford University Press   \n",
       "14             1998                     Back Bay Books   \n",
       "15             2003                             Bantam   \n",
       "16             1997                       HarperTrophy   \n",
       "17             1991      Running Press Book Publishers   \n",
       "18             2001                             Pocket   \n",
       "19             2001                              Knopf   \n",
       "\n",
       "                                            user_hist  \\\n",
       "0   [051513287X, 0515131229, 0312982518, 067103818...   \n",
       "1   [0316955124, 0380727501, 0609809547, 038071089...   \n",
       "2   [0515125490, 0385264267, 0440240913, 067102136...   \n",
       "3   [3404131215, 0373029373, 037311513X, 037316171...   \n",
       "4   [0375700498, 8408002511, 8807102005, 089471796...   \n",
       "5   [0451513355, 0449202917, 0393321096, 055321041...   \n",
       "6   [0671027387, 0345455207, 0312995423, 081251528...   \n",
       "7   [3499134705, 3442420024, 3492226965, 351836987...   \n",
       "8   [840208348X, 2253012726, 0805029516, 058253701...   \n",
       "9   [0425185222, 0373710992, 037371162X, 055325800...   \n",
       "10  [0385510187, 0786868716, 1551667363, 067176074...   \n",
       "11  [8408022393, 0553573926, 0446605891, 880614304...   \n",
       "12  [0375407251, 0060175966, 034541005X, 044022103...   \n",
       "13  [0451179803, 0826412963, 0061031992, 034540803...   \n",
       "14  [0345339703, 034543479X, 0316284955, 068484267...   \n",
       "15  [0330412140, 0060927968, 0156027321, 037540632...   \n",
       "16  [0553157248, 0394800060, 0517187868, 067988468...   \n",
       "17  [0892815256, 0684815001, 0916441822, 080105806...   \n",
       "18  [0373250371, 0312291639, 0312983271, 045140888...   \n",
       "19  [0385323867, 0385308477, 0385323832, 038533392...   \n",
       "\n",
       "                                          hist_rating  labels  rating  \n",
       "0                 [6, 0, 9, 6, 10, 6, 7, 9, 8, 7, 10]       1       7  \n",
       "1                                  [0, 0, 0, 0, 4, 8]       0       0  \n",
       "2                                     [8, 0, 0, 8, 7]       0       0  \n",
       "3   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       0       0  \n",
       "4                                  [0, 0, 0, 0, 0, 0]       0       0  \n",
       "5              [9, 10, 9, 5, 6, 7, 0, 9, 8, 9, 10, 0]       0       4  \n",
       "6                         [7, 9, 6, 0, 0, 6, 0, 6, 9]       0       0  \n",
       "7                                     [6, 0, 7, 0, 7]       1       9  \n",
       "8   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       0       0  \n",
       "9                                     [0, 6, 8, 0, 0]       0       0  \n",
       "10                           [0, 8, 8, 6, 9, 0, 8, 0]       0       0  \n",
       "11  [0, 0, 0, 9, 7, 0, 0, 0, 0, 0, 0, 0, 8, 7, 0, ...       1       9  \n",
       "12                                    [8, 8, 9, 0, 8]       1       9  \n",
       "13  [0, 10, 8, 0, 0, 10, 8, 10, 10, 0, 0, 0, 8, 8,...       0       0  \n",
       "14  [10, 10, 0, 0, 10, 9, 5, 7, 10, 0, 8, 10, 10, ...       1       8  \n",
       "15                                   [0, 0, 10, 0, 9]       1       9  \n",
       "16                                    [8, 0, 8, 6, 7]       1       9  \n",
       "17  [0, 0, 0, 10, 10, 10, 10, 10, 10, 10, 10, 5, 8...       1      10  \n",
       "18                                    [0, 0, 5, 0, 0]       0       0  \n",
       "19                                    [6, 8, 5, 8, 8]       1       8  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "random.shuffle(data_list)\n",
    "df_data = pd.DataFrame(data_list, columns=user_fields + book_fields + [\"user_hist\", \"hist_rating\" , \"labels\", \"rating\"])\n",
    "print(f\"Total number of samples: {len(df_data)}\")\n",
    "\n",
    "df_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train num: 15942\n",
      "Test num: 1772\n"
     ]
    }
   ],
   "source": [
    "# Save train/test in parquet format\n",
    "\n",
    "df_train = df_data[:int(0.9 * len(df_data))].reset_index(drop=True)\n",
    "df_test = df_data[int(0.9 * len(df_data)):].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train num: {len(df_train)}\")\n",
    "print(f\"Test num: {len(df_test)}\")\n",
    "\n",
    "df_train.to_parquet(\n",
    "    os.path.join(target_dir, \"train.parquet.gz\"), \n",
    "    compression=\"gzip\", \n",
    ")\n",
    "df_test.to_parquet(\n",
    "    os.path.join(target_dir, \"test.parquet.gz\"), \n",
    "    compression=\"gzip\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-read for sanity check\n",
    "\n",
    "train_dataset = pd.read_parquet(os.path.join(target_dir, \"train.parquet.gz\"))\n",
    "test_dataset = pd.read_parquet(os.path.join(target_dir, \"test.parquet.gz\"))\n",
    "\n",
    "for (i1, a1), (i2, a2) in zip(df_train.iterrows(), train_dataset.iterrows()):\n",
    "    for field in user_fields + book_fields + [\"labels\"]:\n",
    "        assert not isinstance(a1[field], str) or \"\\t\" not in a1[field]\n",
    "        assert a1[field] == a2[field], (field, a1[field], a2[field])\n",
    "for (i1, a1), (i2, a2) in zip(df_test.iterrows(), test_dataset.iterrows()):\n",
    "    for field in user_fields + book_fields + [\"labels\"]:\n",
    "        assert not isinstance(a1[field], str) or \"\\t\" not in a1[field]\n",
    "        assert a1[field] == a2[field], (field, a1[field], a2[field])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User ID 278858\n",
      "Location 922\n",
      "Age 11\n",
      "ISBN 271375\n",
      "Book title 242152\n",
      "Author 102027\n",
      "Publication year 116\n",
      "Publisher 16807\n"
     ]
    }
   ],
   "source": [
    "# Save the meta data for CTR\n",
    "\n",
    "field_names = user_fields + book_fields\n",
    "\n",
    "feature_count = [len(feature_dict[field]) for field in field_names]\n",
    "\n",
    "feature_offset = [0]\n",
    "for c in feature_count[:-1]:\n",
    "    feature_offset.append(feature_offset[-1] + c)\n",
    "\n",
    "for field in field_names:\n",
    "    print(field, len(feature_dict[field]))\n",
    "\n",
    "meta_data = {\n",
    "    'field_names': field_names,\n",
    "    'feature_count': feature_count,\n",
    "    'feature_dict': feature_dict,\n",
    "    'feature_offset': feature_offset,\n",
    "    'num_ratings': 11\n",
    "}\n",
    "\n",
    "json.dump(meta_data, open(os.path.join(target_dir, 'ctr-meta.json'), 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_dict = json.load(open(os.path.join(target_dir, 'book_dict.json')))\n",
    "meta_data = json.load(open(os.path.join(target_dir, 'ctr-meta.json')))\n",
    "isbn2id = meta_data['feature_dict']['ISBN']\n",
    "id2book = {book_id: [isbn] + book_dict[isbn] for isbn, book_id in isbn2id.items()}\n",
    "json.dump(id2book, open(os.path.join(target_dir, 'id2book.json'), \"w\"), indent=4)\n",
    "json.dump(isbn2id, open(os.path.join(target_dir, 'isbn2id.json'), \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ctr_X (17714, 8)\n",
      "ctr_Y (17714,)\n"
     ]
    }
   ],
   "source": [
    "# Convert df_data to CTR data via feature_dict\n",
    "\n",
    "ctr_X, ctr_Y = [], []\n",
    "for idx, row in df_data.iterrows():\n",
    "    ctr_X.append([feature_dict[field][row[field]] for field in field_names])\n",
    "    ctr_Y.append(int(row[\"labels\"]))\n",
    "\n",
    "\n",
    "ctr_X = np.array(ctr_X)\n",
    "ctr_Y = np.array(ctr_Y)\n",
    "print(\"ctr_X\", ctr_X.shape)\n",
    "print(\"ctr_Y\", ctr_Y.shape)\n",
    "feature_count_np = np.array(feature_count).reshape(1, -1)\n",
    "assert (ctr_X - feature_count_np <= 0).sum() == ctr_X.shape[0] * ctr_X.shape[1]\n",
    "assert (ctr_Y == 0).sum() + (ctr_Y == 1).sum() == ctr_Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_column = {}\n",
    "\n",
    "history_column[\"ID\"] = df_data['user_hist'].tolist()\n",
    "history_column[\"rating\"] = df_data['hist_rating'].tolist()\n",
    "history_column[\"hist length\"] = [len(x) for x in history_column[\"rating\"]]\n",
    "\n",
    "train_num = int(0.9 * len(ctr_X))\n",
    "\n",
    "user_seq = {\n",
    "    \"history ID\": {\n",
    "        \"train\": history_column[\"ID\"][:train_num],\n",
    "        \"test\": history_column[\"ID\"][train_num:],\n",
    "    },\n",
    "    \"history rating\": {\n",
    "        \"train\": history_column[\"rating\"][:train_num],\n",
    "        \"test\": history_column[\"rating\"][train_num:],\n",
    "    },\n",
    "    \"history length\": {\n",
    "        \"train\": history_column[\"hist length\"][:train_num],\n",
    "        \"test\": history_column[\"hist length\"][train_num:],\n",
    "    },\n",
    "}\n",
    "\n",
    "json.dump(user_seq, open(os.path.join(target_dir, \"user_seq.json\"), \"w\"), ensure_ascii=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "user_seq_trunc = {\n",
    "    \"history ID\": {}, \n",
    "    \"history rating\": {}, \n",
    "    \"history mask\": {}, \n",
    "}\n",
    "\n",
    "for hist_name in user_seq:\n",
    "    for split in user_seq[hist_name]:\n",
    "        if hist_name != \"history length\":\n",
    "            user_seq_trunc[hist_name][split] = pad_sequence(\n",
    "                [torch.tensor(x[-60:]) for x in user_seq[hist_name][split]], \n",
    "                batch_first=True, \n",
    "            )\n",
    "        else:\n",
    "            user_seq_trunc[\"history mask\"][split] = pad_sequence(\n",
    "                [torch.ones(min(x, 60)) for x in user_seq[hist_name][split]], \n",
    "                batch_first=True, \n",
    "            )\n",
    "\n",
    "md5_user_seq_trunc = {}\n",
    "for hist_name in user_seq_trunc:\n",
    "    md5_user_seq_trunc[hist_name] = {}\n",
    "    for split in user_seq_trunc[hist_name]:\n",
    "        md5_user_seq_trunc[hist_name][split] = user_seq_trunc[hist_name][split].tolist()\n",
    "        print(hist_name, split, user_seq_trunc[hist_name][split].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CTR data\n",
    "\n",
    "with h5py.File(os.path.join(target_dir, f'ctr.h5'), 'w') as hf:\n",
    "    hf.create_dataset('train data', data=ctr_X[:int(0.9 * len(ctr_X)), :])\n",
    "    hf.create_dataset('test data', data=ctr_X[int(0.9 * len(ctr_X)):, :])\n",
    "    hf.create_dataset('train label', data=ctr_Y[:int(0.9 * len(ctr_X))])\n",
    "    hf.create_dataset('test label', data=ctr_Y[int(0.9 * len(ctr_X)):])\n",
    "    for hist_name in user_seq_trunc:\n",
    "        for split in user_seq_trunc[hist_name]:\n",
    "            hf.create_dataset(f\"{split} {hist_name}\", data=user_seq_trunc[hist_name][split])\n",
    "\n",
    "\n",
    "with h5py.File(os.path.join(target_dir, f'ctr.h5'), 'r') as hf:\n",
    "    assert (ctr_X - np.concatenate([hf['train data'][:], hf['test data'][:]], axis=0)).sum() == 0\n",
    "    assert (ctr_Y - np.concatenate([hf['train label'][:], hf['test label'][:]], axis=0)).sum() == 0\n",
    "    for hist_name in user_seq_trunc:\n",
    "        for split in user_seq_trunc[hist_name]:\n",
    "            assert (user_seq_trunc[hist_name][split] - hf[f\"{split} {hist_name}\"][:]).sum() == 0    \n",
    "\n",
    "    x = hf['train data'][:]\n",
    "    assert (x - ctr_X[:int(0.9 * len(ctr_X)), :]).sum() == 0\n",
    "    print(f'train data: {x.shape}')\n",
    "    \n",
    "    x = hf['test data'][:]\n",
    "    assert (x - ctr_X[int(0.9 * len(ctr_X)):, :]).sum() == 0\n",
    "    print(f'test data: {x.shape}')\n",
    "    x = hf['train label'][:]\n",
    "    assert (x - ctr_Y[:int(0.9 * len(ctr_X))]).sum() == 0\n",
    "    print(f'train label: {x.shape}')\n",
    "    x = hf['test label'][:]\n",
    "    assert (x - ctr_Y[int(0.9 * len(ctr_X)):]).sum() == 0\n",
    "    print(f'test label: {x.shape}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final check: ensure each row from tsv and ctr is matched\n",
    "\n",
    "train_dataset = pd.read_parquet(os.path.join(target_dir, 'train.parquet.gz'))\n",
    "test_dataset = pd.read_parquet(os.path.join(target_dir, 'test.parquet.gz')).reset_index(drop=True)\n",
    "\n",
    "\n",
    "with h5py.File(os.path.join(target_dir, f'ctr.h5'), 'r') as hf:\n",
    "    train_x = hf['train data'][:]\n",
    "    train_y = hf['train label'][:]\n",
    "    test_x = hf['test data'][:]\n",
    "    test_y = hf['test label'][:]\n",
    "\n",
    "for idx, row in train_dataset.iterrows():\n",
    "    for fi, field in enumerate(field_names):\n",
    "        assert feature_dict[field][row[field]] == train_x[idx, fi]\n",
    "    assert int(row[\"labels\"]) == train_y[idx]\n",
    "\n",
    "for idx, row in test_dataset.iterrows():\n",
    "    for fi, field in enumerate(field_names):\n",
    "        assert feature_dict[field][row[field]] == test_x[idx, fi]\n",
    "    assert int(row[\"labels\"]) == test_y[idx]\n",
    "\n",
    "print(\"Pass final check.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
